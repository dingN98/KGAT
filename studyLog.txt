2022.5.18
不知道为啥，commit的文件只要达到几十M，就会push失败，所以我只push了代码，没有push数据。

论文速看：

KGAT模型主要由三个部分组成:
1)嵌入层，通过保留CKG的结构，将每个节点参数化为一个向量;
2)注意嵌入传播层，递归地传播来自节点邻居的嵌入，更新节点表示，并在传播过程中利用知识感知的注意机制学习每个邻居的权值;
3)预测层，将来自所有传播层的用户和项的表示集合起来，输出预测的匹配分数。

【嵌入层】使用【TransR】模型将【实体】和【关系】向量化
【注意嵌入传播层】 权重的计算、tail乘以权重（结果记作A)、head和A的聚合、多跳、多跳的聚合
【预测层】将user的多跳聚合结果 和 item的多跳聚合结果 内积，作为向user推荐item的分数

明天看代码。
ps:今天跑了2公里，用了11分半。

2022.5.19
和龙王谈了下，他在华为南研所上班，一年30万以上（含年终奖10万），会加班。在牛首山附近（云望府），2.79万一平。租房一年五万，和女朋友住一起。
和老姐谈了下，江苏银行技术岗，会加班到晚上八九点，年薪不及华为，但是更加稳定。姐夫刚毕业也没有几十万，其他途径涨到这么高（七十万以上）。

关于今天的小论文，看代码，看他是：
1、怎么构建联合知识图谱CKG
2、那几个公式、模型的构建
3、怎么和基线比较性能

只能下午开始了，上午都在聊天。

一个执行命令
python Main.py --model_type kgat --alg_type bi --dataset last-fm --regs [1e-5,1e-5] --layer_size [64,32,16] --embed_size 64 --lr 0.0001 --epoch 1000 --verbose 50 --save_flag 1 --pretrain -1 --batch_size 1024 --node_dropout [0.1] --mess_dropout [0.1,0.1,0.1] --use_att True --use_kge True
预计跑完1000个epoch，需要2.7天

Main.py

1、首先是加载数据集 load_data.py
（1）得到训练集和测试集  self._load_ratings()
self.train_data, self.train_user_dict = self._load_ratings(train_file)     train_file = path + '/train.txt'
train.txt  每一行是 【userid,item1,itme2,...】   train_data 是一个矩阵，每一行是【userid,itemid】  train_user_dict 是一个dict()，key是userID,value是这个user交互的items【item1，item2，...】
self.test_data, self.test_user_dict = self._load_ratings(test_file)  类似
（2）得到知识图谱相关数据 self._load_kg()
kg_dict  是一个字典  kg[head].append((tail, relation))
relation_dict   是一个字典  rd[relation].append((head, tail))
（3）生成 batch_size 大小的正样品和负样品  _generate_train_cf_batch()
return users, pos_items, neg_items
其中，users 是 batch_size 大小的[]
pos_items 是 batch_size * num 的矩阵， 是指 train_user_dict 里面涉及到的数据
neg_items 是 train_user_dict 里面没提到的数据

KGAT.py


1、参数转换 def _parse_args(self, data_config, pretrain_data, args)
首先获取 n_users n_items n_entities n_relations
然后是其他数据   来自   loader_kgat.py
（1）根据 user-item 交互数据 生成 稀疏的邻接矩阵

# generate the sparse adjacency matrices for user-item interaction & relational kg data.
self.adj_list, self.adj_r_list = self._get_relational_adj_list()



Epoch 0 [261.7s]: train==[789.20233=39.57340 + 749.46057 + 0.16889]
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
        convert ratings into adj mat done.
        convert ratings into adj mat done.
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
        convert 20 relational triples into adj mat done. @0.4528s
        convert 20 relational triples into adj mat done. @0.4299s

2022.5.20 玩
2022.5.21 玩
2022.5.22
继续干活儿
上回解析了Main.py里的主要逻辑：
生成训练集、测试集、知识图谱相关数据、batch_size大小的正样品和负样品

在研究 KGAT.py之前，需要研究 loader_kgat.py  它为KGAT.py提供数据来源

loader_kgat.py   做的准备工作如下：

# generate the sparse adjacency matrices for user-item interaction & relational kg data.
# 生成稀疏的邻接矩阵（user-item交互矩阵、KG关系），adj_list 包含所有的user-item的交互矩阵  还有逆矩阵，adj_r_list 包含所有的  关系id
# 用稀疏矩阵变量adj_mat_list存储CF与KG的连通信息。
self.adj_list, self.adj_r_list = self._get_relational_adj_list()

# generate the sparse laplacian matrices.
# 生成稀疏的拉普拉斯矩阵，主要是上一步得到的稀疏CF及KG连通矩阵进行归一化操作。
# 这里重点说一下为啥需要归一化：原始的连通矩阵是没有归一化的，如果没有归一化的连通矩阵与特征矩阵相乘会改变特征的原本分布，产生一些不可预测的问题，所以需要对连通矩阵进行标准化处理。
self.lap_list = self._get_relational_lap_list()

# generate the triples dictionary, key is 'head', value is '(tail, relation)'.
# 生成三元组字典： all_kg_dict[head].append((tail, relation))
self.all_kg_dict = self._get_all_kg_dict()

# 获取所有的 head列表，relation列表、tail列表，data列表
self.all_h_list, self.all_r_list, self.all_t_list, self.all_v_list = self._get_all_kg_data()

接着研究核心代码 KGAT.py

# 解析参数，包含 n_users、n_items、若干邻接矩阵 等等
self._parse_args(data_config, pretrain_data, args)

# Create Placeholder for Input Data & Dropout.
self._build_inputs()

# Create Model Parameters for CF & KGE parts.
# 构建所有的权重  all_weights = dict()  如果存在预训练数据，那么使用，否则使用 xavier 初始化随机数据
# 权重包括：用户嵌入 user_embed 【n_users * emb_dim】实体嵌入 entity_embed 【n_entities * emb_dim】 物品嵌入 item_embed 关系嵌入  relation_embed
# ??? trans_W   tf.Variable(initializer([self.n_relations, self.emb_dim, self.kge_dim]))   ??? 这里需要研究下
# ??? gc、bi、mlp三层是啥???
# 执行命令 python Main.py 里有一个参数  --layer_size [64,32,16]
而 self.weight_size = eval(args.layer_size)   self.n_layers = len(self.weight_size)  这里的layer_size  就是 3
# 然后遍历这三层layer，继续往 all_weights 里加入参数  三层  gc bi mlp
# 三层 W 其实对应着  emb_dim * 64 、 64*32 、(2*32) * 16
# 三层 b 对应着 1*64、1*32、1*16
self.weights = self._build_weights()



2022.5.23
今天白天都在和 htw 一起搞服务器，成功装到机架上了，但是还是IP有问题。
晚上继续学习小论文代码！！！

# Compute Graph-based Representations of All Users & Items & KG Entities via Message-Passing Mechanism of Graph Neural Networks.
# 使用不同的卷积层，通过KG的消息传递机制，计算所有的 user、item、entity的 表示
# Different Convolutional Layers: bi  、 gcn 、 graphsage  这其实是 当前实体的信息 和 邻域实体 信息聚合的三种方式
# python Main.py --model_type kgat --alg_type bi  可见论文作者默认使用 bi 卷积层，因为从论文的实验结果来看，bi聚合器的 指标最好
# 三个核心函数的核心不同点代码
# 1、 _create_bi_interaction_embed()    fBi-Interaction =LeakyReLU?W1(eh + eNh )?+ LeakyReLU?W2(eh ⊙ eNh )?,
# add_embeddings = ego_embeddings + side_embeddings
# bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)
# 2、_create_gcn_embed()    fGCN = LeakyReLU?W(eh + eNh )?
# embeddings = tf.concat(temp_embed, 0)
# 3、_create_graphsage_embed()     fGraphSage = LeakyReLU?W(eh ||eNh )?
#  embeddings = tf.concat([pre_embeddings, embeddings], 1)
self._build_model_phase_I()

# Optimize Recommendation (CF) Part via BPR Loss.
# 对应论文 3.4 节  LCF =X(u,i, j)∈ O− ln σ ? ˆy(u, i) − ˆy(u, j)?
self._build_loss_phase_I()

# Compute Knowledge Graph Embeddings via TransR.
# 计算 实体的 TransR 嵌入
self._build_model_phase_II()

# Optimize KGE Part via BPR Loss.
# 优化 KGE 的损失函数
self._build_loss_phase_II()

# 打印静态参数
self._statistics_params()

重点来了！！！
Main.py 继续 Train
是否使用 KGE 模型 和 是否使用注意力机制  是可以调节的！
264行    if args.use_kge is True:
278行    if args.use_att is True:

for epoch in range(args.epoch):
    phase 1: to train the recommender.
    phase 2: to train the KGE method & update the attentive Laplacian matrix.
    Test.
    Performance logging.

明天继续！


