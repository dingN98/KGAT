2022.5.18
不知道为啥，commit的文件只要达到几十M，就会push失败，所以我只push了代码，没有push数据。

论文速看：

KGAT模型主要由三个部分组成:
1)嵌入层，通过保留CKG的结构，将每个节点参数化为一个向量;
2)注意嵌入传播层，递归地传播来自节点邻居的嵌入，更新节点表示，并在传播过程中利用知识感知的注意机制学习每个邻居的权值;
3)预测层，将来自所有传播层的用户和项的表示集合起来，输出预测的匹配分数。

【嵌入层】使用【TransR】模型将【实体】和【关系】向量化
【注意嵌入传播层】 权重的计算、tail乘以权重（结果记作A)、head和A的聚合、多跳、多跳的聚合
【预测层】将user的多跳聚合结果 和 item的多跳聚合结果 内积，作为向user推荐item的分数

明天看代码。
ps:今天跑了2公里，用了11分半。

2022.5.19
和龙王谈了下，他在华为南研所上班，一年30万以上（含年终奖10万），会加班。在牛首山附近（云望府），2.79万一平。租房一年五万，和女朋友住一起。
和老姐谈了下，江苏银行技术岗，会加班到晚上八九点，年薪不及华为，但是更加稳定。姐夫刚毕业也没有几十万，其他途径涨到这么高（七十万以上）。

关于今天的小论文，看代码，看他是：
1、怎么构建联合知识图谱CKG
2、那几个公式、模型的构建
3、怎么和基线比较性能

只能下午开始了，上午都在聊天。

一个执行命令
python Main.py --model_type kgat --alg_type bi --dataset last-fm --regs [1e-5,1e-5] --layer_size [64,32,16] --embed_size 64 --lr 0.0001 --epoch 1000 --verbose 50 --save_flag 1 --pretrain -1 --batch_size 1024 --node_dropout [0.1] --mess_dropout [0.1,0.1,0.1] --use_att True --use_kge True
预计跑完1000个epoch，需要2.7天

Main.py

1、首先是加载数据集 load_data.py
（1）得到训练集和测试集  self._load_ratings()
self.train_data, self.train_user_dict = self._load_ratings(train_file)     train_file = path + '/train.txt'
train.txt  每一行是 【userid,item1,itme2,...】   train_data 是一个矩阵，每一行是【userid,itemid】  train_user_dict 是一个dict()，key是userID,value是这个user交互的items【item1，item2，...】
self.test_data, self.test_user_dict = self._load_ratings(test_file)  类似
（2）得到知识图谱相关数据 self._load_kg()
kg_dict  是一个字典  kg[head].append((tail, relation))
relation_dict   是一个字典  rd[relation].append((head, tail))
（3）生成 batch_size 大小的正样品和负样品  _generate_train_cf_batch()
return users, pos_items, neg_items
其中，users 是 batch_size 大小的[]
pos_items 是 batch_size * num 的矩阵， 是指 train_user_dict 里面涉及到的数据
neg_items 是 train_user_dict 里面没提到的数据

KGAT.py


1、参数转换 def _parse_args(self, data_config, pretrain_data, args)
首先获取 n_users n_items n_entities n_relations
然后是其他数据   来自   loader_kgat.py
（1）根据 user-item 交互数据 生成 稀疏的邻接矩阵

# generate the sparse adjacency matrices for user-item interaction & relational kg data.
self.adj_list, self.adj_r_list = self._get_relational_adj_list()



Epoch 0 [261.7s]: train==[789.20233=39.57340 + 749.46057 + 0.16889]
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
        convert ratings into adj mat done.
        convert ratings into adj mat done.
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
        convert 20 relational triples into adj mat done. @0.4528s
        convert 20 relational triples into adj mat done. @0.4299s

2022.5.20 玩
2022.5.21 玩
2022.5.22
继续干活儿
上回解析了Main.py里的主要逻辑：
生成训练集、测试集、知识图谱相关数据、batch_size大小的正样品和负样品

在研究 KGAT.py之前，需要研究 loader_kgat.py  它为KGAT.py提供数据来源

loader_kgat.py   做的准备工作如下：

# generate the sparse adjacency matrices for user-item interaction & relational kg data.
# 生成稀疏的邻接矩阵（user-item交互矩阵、KG关系），adj_list 包含所有的user-item的交互矩阵  还有逆矩阵，adj_r_list 包含所有的  关系id
# 用稀疏矩阵变量adj_mat_list存储CF与KG的连通信息。
self.adj_list, self.adj_r_list = self._get_relational_adj_list()

# generate the sparse laplacian matrices.
# 生成稀疏的拉普拉斯矩阵，主要是上一步得到的稀疏CF及KG连通矩阵进行归一化操作。
# 这里重点说一下为啥需要归一化：原始的连通矩阵是没有归一化的，如果没有归一化的连通矩阵与特征矩阵相乘会改变特征的原本分布，产生一些不可预测的问题，所以需要对连通矩阵进行标准化处理。
self.lap_list = self._get_relational_lap_list()

# generate the triples dictionary, key is 'head', value is '(tail, relation)'.
# 生成三元组字典： all_kg_dict[head].append((tail, relation))
self.all_kg_dict = self._get_all_kg_dict()

# 获取所有的 head列表，relation列表、tail列表，data列表
self.all_h_list, self.all_r_list, self.all_t_list, self.all_v_list = self._get_all_kg_data()

接着研究核心代码 KGAT.py

# 解析参数，包含 n_users、n_items、若干邻接矩阵 等等
self._parse_args(data_config, pretrain_data, args)

# Create Placeholder for Input Data & Dropout.
self._build_inputs()

# Create Model Parameters for CF & KGE parts.
# 构建所有的权重  all_weights = dict()  如果存在预训练数据，那么使用，否则使用 xavier 初始化随机数据
# 权重包括：用户嵌入 user_embed 【n_users * emb_dim】实体嵌入 entity_embed 【n_entities * emb_dim】 物品嵌入 item_embed 关系嵌入  relation_embed
# ??? trans_W   tf.Variable(initializer([self.n_relations, self.emb_dim, self.kge_dim]))   ??? 这里需要研究下
# ??? gc、bi、mlp三层是啥???
# 执行命令 python Main.py 里有一个参数  --layer_size [64,32,16]
而 self.weight_size = eval(args.layer_size)   self.n_layers = len(self.weight_size)  这里的layer_size  就是 3
# 然后遍历这三层layer，继续往 all_weights 里加入参数  三层  gc bi mlp
# 三层 W 其实对应着  emb_dim * 64 、 64*32 、(2*32) * 16
# 三层 b 对应着 1*64、1*32、1*16
self.weights = self._build_weights()



2022.5.23
今天白天都在和 htw 一起搞服务器，成功装到机架上了，但是还是IP有问题。
晚上继续学习小论文代码！！！

# Compute Graph-based Representations of All Users & Items & KG Entities via Message-Passing Mechanism of Graph Neural Networks.
# 使用不同的卷积层，通过KG的消息传递机制，计算所有的 user、item、entity的 表示
# Different Convolutional Layers: bi  、 gcn 、 graphsage  这其实是 当前实体的信息 和 邻域实体 信息聚合的三种方式
# python Main.py --model_type kgat --alg_type bi  可见论文作者默认使用 bi 卷积层，因为从论文的实验结果来看，bi聚合器的 指标最好
# 三个核心函数的核心不同点代码
# 1、 _create_bi_interaction_embed()    fBi-Interaction =LeakyReLU?W1(eh + eNh )?+ LeakyReLU?W2(eh ⊙ eNh )?,
# add_embeddings = ego_embeddings + side_embeddings
# bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)
# 2、_create_gcn_embed()    fGCN = LeakyReLU?W(eh + eNh )?
# embeddings = tf.concat(temp_embed, 0)
# 3、_create_graphsage_embed()     fGraphSage = LeakyReLU?W(eh ||eNh )?
#  embeddings = tf.concat([pre_embeddings, embeddings], 1)
self._build_model_phase_I()

# Optimize Recommendation (CF) Part via BPR Loss.
# 对应论文 3.4 节  LCF =X(u,i, j)∈ O− ln σ ? ˆy(u, i) − ˆy(u, j)?
self._build_loss_phase_I()

# Compute Knowledge Graph Embeddings via TransR.
# 计算 实体的 TransR 嵌入
self._build_model_phase_II()

# Optimize KGE Part via BPR Loss.
# 优化 KGE 的损失函数
self._build_loss_phase_II()

# 打印静态参数
self._statistics_params()

重点来了！！！
Main.py 继续 Train
是否使用 KGE 模型 和 是否使用注意力机制  是可以调节的！
264行    if args.use_kge is True:
278行    if args.use_att is True:

for epoch in range(args.epoch):
    phase 1: to train the recommender.
    phase 2: to train the KGE method & update the attentive Laplacian matrix.
    Test.
    Performance logging.

明天继续！

2022.5.24
白天在搞解决 HDFS 的 bug，晚上继续搞论文

首先要搞明白
1、是不是一定要用KGE模型
（1）这个KGE需要的训练数据怎么生成?
loader_kgat.py  里面的 _generate_train_A_batch()方法
heads, relations, pos_tails, neg_tails = self._generate_train_A_batch()

需要知道这个方法是怎么生成 batch_size 大小的训练数据的
首先，需要知道 all_kg_dict 里记录了所有的知识图谱三元组  all_kg_dict[head].append((tail, relation))
然后，这个 all_kg_dict 里出现的三元组 (h,r,t) 是 pos训练数据， 里面没出现的是 neg训练数据
怎么从 all_kg_dict 里获取这两种数据？ 对于 pos数据，直接从 all_kg_dict 随机选取 batch_size 大小的 数据即可，即为 heads, relations, pos_tails
然后对于每一个 header ，随机生成一个 不在 all_dict[header] 里出现过得 tail，就是 neg_tail  然后最后得到  neg_tails

（2）怎么训练KGE
首先将 self._generate_train_A_batch() 生成的数据构造成 feed_dict 的字典格式
feed_dict = {
            model.h: batch_data['heads'],
            model.r: batch_data['relations'],
            model.pos_t: batch_data['pos_tails'],
            model.neg_t: batch_data['neg_tails'],

        }
注意，这里的例如 model.h  会覆盖 例如model是KGAT里定义的  self.h
然后就是，这里的 self.h 只是 KG 里 head 节点的 id
定义的 self.h_e  才是 h 的 嵌入，初始的嵌入是 随机生成的，二者通过下面这个函数建立关系：
self.h_e, self.r_e, self.pos_t_e, self.neg_t_e = self._get_kg_inference(self.h, self.r, self.pos_t, self.neg_t)
以 head 为例   h_e = tf.nn.embedding_lookup(embeddings, h)   一开始 h 对应的 h_e  是随机生成的

然后才是训练
_, batch_loss, batch_kge_loss, batch_reg_loss = model.train_A(sess, feed_dict=feed_dict)
train_A() 的具体工作
sess.run([self.opt2, self.loss2, self.kge_loss2, self.reg_loss2], feed_dict)
和论文里公式相对应的代码部分如下：
kg_score = tf.reduce_sum(tf.square((h_e + r_e - t_e)), 1, keepdims=True)
kg_loss = tf.reduce_mean(tf.nn.softplus(-(neg_kg_score - pos_kg_score)))

其实，TransR模型的训练，需要将 h 和 t 映射到 r 的维度，然后再执行 h+r-t 这种。好像论文作者为了简化计算，设置的 h、r、t 的维度都是 64      --embed_size 64
不对，在 _get_kg_inference(self, h, r, pos_t, neg_t) 这个函数里，有这个
self.weights['trans_W']
感觉就是 h 到 r 的维度转换矩阵，这个问题留给明天研究。

（3）终于到了最终问题，要不要使用KGE模型
我觉得要解决这个问题，得先解决下一个问题。就是本文作者除了使用了KGE，还使用了基于注意力机制的信息传播和信息聚合，这个东西的作用是啥？如果只用这个，不用KGE的影响。


2、是不是一定要用注意力机制
基于 user-item-rate 做的训练
feed_dict = {
            model.users: batch_data['users'],
            model.pos_items: batch_data['pos_items'],
            model.neg_items: batch_data['neg_items'],

            model.mess_dropout: eval(self.args.mess_dropout),
            model.node_dropout: eval(self.args.node_dropout),
        }
_, batch_loss, batch_base_loss, batch_kge_loss, batch_reg_loss = model.train(sess, feed_dict=feed_dict)

loss计算公式
pos_scores = tf.reduce_sum(tf.multiply(self.u_e, self.pos_i_e), axis=1)
neg_scores = tf.reduce_sum(tf.multiply(self.u_e, self.neg_i_e), axis=1)
base_loss = tf.reduce_mean(tf.nn.softplus(-(pos_scores - neg_scores)))

我感觉 利用 用户的rating 来训练 ，也是在训练 u_e 和 pos_i_e  等这种嵌入，使得 u_e 和 pos_ie 的乘积之和 尽可能大。
因此和 KGE （训练 h_e 、 r_e 、 t_e）的区别是啥呢？
这个问题留给明天解决吧。。。

2022.5.25
要解决俩遗留问题
1、KGE的权重问题  self.weights['trans_W']
2、基于user-item的训练  和 基于 h-r-t 的训练的区别

